---
title: Woorden lijst Quantum machine learning Library
description: Woorden lijst met Quantum machine learning-voor waarden
author: alexeib2
ms.author: alexeib
ms.date: 2/27/2020
ms.topic: article
uid: microsoft.quantum.libraries.machine-learning.training
no-loc:
- 'Q#'
- '$$v'
ms.openlocfilehash: 476e93e3737dee6ad8f3a97e8ffbcfb9b0012ee1
ms.sourcegitcommit: 29e0d88a30e4166fa580132124b0eb57e1f0e986
ms.translationtype: MT
ms.contentlocale: nl-NL
ms.lasthandoff: 10/27/2020
ms.locfileid: "92691516"
---
# <a name="quantum-machine-learning-glossary"></a><span data-ttu-id="31b8d-103">Woorden lijst Quantum Machine Learning</span><span class="sxs-lookup"><span data-stu-id="31b8d-103">Quantum Machine Learning glossary</span></span>

<span data-ttu-id="31b8d-104">De training van een circuit-georiënteerde Quantum classificeerder is een proces waarbij veel bewegende onderdelen zijn die dezelfde (of iets grotere) hoeveelheid kalibratie nodig hebben met een proef versie en fout als opleiding van traditionele classificaties.</span><span class="sxs-lookup"><span data-stu-id="31b8d-104">Training of a circuit-centric quantum classifier is a process with many moving parts that require the same (or slightly larger) amount of calibration by trial and error as training of traditional classifiers.</span></span> <span data-ttu-id="31b8d-105">Hier worden de belangrijkste concepten en ingrediënten van dit trainings proces gedefinieerd.</span><span class="sxs-lookup"><span data-stu-id="31b8d-105">Here we define the main concepts and ingredients of this training process.</span></span>

## <a name="trainingtesting-schedules"></a><span data-ttu-id="31b8d-106">Planningen voor training en tests</span><span class="sxs-lookup"><span data-stu-id="31b8d-106">Training/testing schedules</span></span>

<span data-ttu-id="31b8d-107">In de context van een classificatie training wordt in een *planning* een subset van gegevens voorbeelden beschreven in een algemene training of testset.</span><span class="sxs-lookup"><span data-stu-id="31b8d-107">In the context of classifier training a *schedule* describes a subset of data samples in an overall training or testing set.</span></span> <span data-ttu-id="31b8d-108">Een planning wordt meestal gedefinieerd als een verzameling van voor beelden van indices.</span><span class="sxs-lookup"><span data-stu-id="31b8d-108">A schedule is usually defined as a collection of sample indices.</span></span>

## <a name="parameterbias-scores"></a><span data-ttu-id="31b8d-109">Para meter/bias-scores</span><span class="sxs-lookup"><span data-stu-id="31b8d-109">Parameter/bias scores</span></span>

<span data-ttu-id="31b8d-110">Op basis van een para meter voor de kandidaten en een classifier-afwijking wordt de *validatie Score* gemeten ten opzichte van een gekozen validatie schema en wordt aangegeven door een aantal onvolledige classificaties voor alle voor beelden in de planning S.</span><span class="sxs-lookup"><span data-stu-id="31b8d-110">Given a candidate parameter vector and a classifier bias, their *validation score* is measured relative to a chosen validation schedule S and is expressed by a number of misclassifications counted over all the samples in the schedule S.</span></span>

## <a name="hyperparameters"></a><span data-ttu-id="31b8d-111">Hyper parameters</span><span class="sxs-lookup"><span data-stu-id="31b8d-111">Hyperparameters</span></span>

<span data-ttu-id="31b8d-112">Het model trainings proces is onderworpen aan bepaalde vooraf ingestelde waarden met de naam *Hyper parameters* :</span><span class="sxs-lookup"><span data-stu-id="31b8d-112">The model training process is governed by certain pre-set values called *hyperparameters* :</span></span>

### <a name="learning-rate"></a><span data-ttu-id="31b8d-113">Leersnelheid</span><span class="sxs-lookup"><span data-stu-id="31b8d-113">Learning rate</span></span>

<span data-ttu-id="31b8d-114">Het is een van de Key Hyper parameters.</span><span class="sxs-lookup"><span data-stu-id="31b8d-114">It is one of the key hyperparameters.</span></span> <span data-ttu-id="31b8d-115">Hiermee wordt gedefinieerd hoeveel huidige stochastische verlopen schatting van invloed is op de parameter update.</span><span class="sxs-lookup"><span data-stu-id="31b8d-115">It defines how much current stochastic gradient estimate impacts the parameter update.</span></span> <span data-ttu-id="31b8d-116">De grootte van de Delta parameter update verschilt in verhouding tot het leer tempo.</span><span class="sxs-lookup"><span data-stu-id="31b8d-116">The size of parameter update delta is proportional to the learning rate.</span></span> <span data-ttu-id="31b8d-117">De waarden voor een kortere leer factor leiden tot tragere parameter ontwikkeling en langzamere convergentie, maar buitensporig grote waarden van LR kunnen de convergentie helemaal verstoren, omdat de verloop Daal nooit wordt doorgevoerd in een bepaald lokaal minimum.</span><span class="sxs-lookup"><span data-stu-id="31b8d-117">Smaller learning rate values lead to slower parameter evolution and slower convergence, but excessively large values of LR may break the convergence altogether as the gradient descent never commits to a particular local minimum.</span></span> <span data-ttu-id="31b8d-118">Hoewel het leer tempo op een zekere hoogte adaptief wordt aangepast door het trainings algoritme, is het belang rijk dat u een goede initiële waarde selecteert.</span><span class="sxs-lookup"><span data-stu-id="31b8d-118">While learning rate is adaptively adjusted by the training algorithm to some extent, selecting a good initial value for it is important.</span></span> <span data-ttu-id="31b8d-119">Een gebruikelijke standaard waarde voor het leer tempo is 0,1.</span><span class="sxs-lookup"><span data-stu-id="31b8d-119">A usual default initial value for learning rate is 0.1.</span></span> <span data-ttu-id="31b8d-120">Het selecteren van de beste waarde voor het trainings tempo is een fijne illustratie (zie bijvoorbeeld sectie 4,3 van Goodfellow et al. ' diep leren ', MIT Press, 2017).</span><span class="sxs-lookup"><span data-stu-id="31b8d-120">Selecting the best value of learning rate is a fine art (see, for example, section 4.3 of Goodfellow et al.,"Deep learning", MIT Press, 2017).</span></span>

### <a name="minibatch-size"></a><span data-ttu-id="31b8d-121">Grootte van Minibatch</span><span class="sxs-lookup"><span data-stu-id="31b8d-121">Minibatch size</span></span>

<span data-ttu-id="31b8d-122">Hiermee definieert u hoeveel gegevens voorbeelden worden gebruikt voor één schatting van de stochastische kleur overgang.</span><span class="sxs-lookup"><span data-stu-id="31b8d-122">Defines how many data samples is used for a single estimation of stochastic gradient.</span></span> <span data-ttu-id="31b8d-123">Grotere waarden van de grootte van minibatch leiden doorgaans tot robuustere en meer monotone convergentie, maar kunnen het trainings proces vertragen, omdat de kosten van een raming van één kleur overgang evenredig zijn met de minimatch grootte.</span><span class="sxs-lookup"><span data-stu-id="31b8d-123">Larger values of minibatch size generally lead to more robust and more monotonic convergence but can potentially slow down the training process, as the cost of any one gradient estimation is proportional to the minimatch size.</span></span> <span data-ttu-id="31b8d-124">Een gebruikelijke standaard waarde voor de grootte van de minibatch is 10.</span><span class="sxs-lookup"><span data-stu-id="31b8d-124">A usual default value for the minibatch size is 10.</span></span>

### <a name="training-epochs-tolerance-gridlocks"></a><span data-ttu-id="31b8d-125">Trainings-epoches, tolerantie, gridlocks</span><span class="sxs-lookup"><span data-stu-id="31b8d-125">Training epochs, tolerance, gridlocks</span></span>

<span data-ttu-id="31b8d-126">' Epoche ' betekent één volledige Pass-Through de geplande trainings gegevens.</span><span class="sxs-lookup"><span data-stu-id="31b8d-126">"Epoch" means one complete pass through the scheduled training data.</span></span>
<span data-ttu-id="31b8d-127">Het maximum aantal epochen per trainings thread (zie hieronder) moet worden afgelimiteerd.</span><span class="sxs-lookup"><span data-stu-id="31b8d-127">The maximum number of epochs per a training thread (see below) should be capped.</span></span> <span data-ttu-id="31b8d-128">De trainings thread is gedefinieerd om te beëindigen (met de best bekende kandidaat-para meters) wanneer het maximum aantal epoches is uitgevoerd.</span><span class="sxs-lookup"><span data-stu-id="31b8d-128">The training thread is defined to terminate (with the best known candidate parameters) when the maximum number of epochs has been run.</span></span> <span data-ttu-id="31b8d-129">Deze training zou echter eerder worden beëindigd wanneer het classificatie niveau voor het validatie schema lager is dan de gekozen tolerantie.</span><span class="sxs-lookup"><span data-stu-id="31b8d-129">However such training would terminate earlier when misclassification rate on validation schedule falls below a chosen tolerance.</span></span> <span data-ttu-id="31b8d-130">Stel bijvoorbeeld dat de tolerantie van een onwaarschijnlijke classificatie 0,01 (1%) is. als op de validatieset van 2000-voor beelden wordt aangegeven dat er minder dan 20 misindelingen worden weer gegeven, is het tolerantie niveau bereikt.</span><span class="sxs-lookup"><span data-stu-id="31b8d-130">Suppose, for example, that misclassification tolerance is 0.01 (1%); if on validation set of 2000 samples we are seeing fewer than 20 misclassifications, then the tolerance level has been achieved.</span></span> <span data-ttu-id="31b8d-131">Een trainings thread wordt voor tijdig beëindigd als de validatie Score van het kandidaten model geen verbetering heeft vertoond ten opzichte van verschillende opeenvolgende epoches (een gridlock).</span><span class="sxs-lookup"><span data-stu-id="31b8d-131">A training thread also terminates prematurely if the validation score of the candidate model has not shown any improvement over several consecutive epochs (a gridlock).</span></span> <span data-ttu-id="31b8d-132">De logica voor de gridlock-beëindiging is momenteel hardcoded.</span><span class="sxs-lookup"><span data-stu-id="31b8d-132">The logic for the gridlock termination is currently hardcoded.</span></span>

### <a name="measurements-count"></a><span data-ttu-id="31b8d-133">Aantal metingen</span><span class="sxs-lookup"><span data-stu-id="31b8d-133">Measurements count</span></span>

<span data-ttu-id="31b8d-134">Het schatten van de trainings-en validatie scores en de onderdelen van de stochastische kleur overgang op een Quantum apparaat bedragen voor het schatten van de Quantum status overlap pingen waarvoor meerdere metingen van de juiste observables zijn vereist.</span><span class="sxs-lookup"><span data-stu-id="31b8d-134">Estimating the training/validation scores and the components of the stochastic gradient on a quantum device amounts to estimating quantum state overlaps that requires multiple measurements of the appropriate observables.</span></span> <span data-ttu-id="31b8d-135">Het aantal metingen moet worden geschaald als $O (1/\ Epsilon ^ 2) $ waarbij $ \epsilon $ de gewenste schattings fout is.</span><span class="sxs-lookup"><span data-stu-id="31b8d-135">The number of measurements should scale as $O(1/\epsilon^2)$ where $\epsilon$ is the desired estimation error.</span></span>
<span data-ttu-id="31b8d-136">Als vuist regel kan het aantal oorspronkelijke metingen ongeveer $1/\ mbox {tolerantie} ^ 2 $ zijn (Zie de definitie van tolerantie in de vorige alinea).</span><span class="sxs-lookup"><span data-stu-id="31b8d-136">As a rule of thumb, the initial measurements count could be approximately $1/\mbox{tolerance}^2$ (see definition of tolerance in the previous paragraph).</span></span> <span data-ttu-id="31b8d-137">Een voor waarde is dat het aantal metingen omhoog moet worden gewijzigd als de verloop Daal te schokkerig lijkt en de convergentie te moeilijk te verloopt.</span><span class="sxs-lookup"><span data-stu-id="31b8d-137">One would need to revise the measurement count upward if the gradient descent appears to be too erratic and convergence too hard to achieve.</span></span>

### <a name="training-threads"></a><span data-ttu-id="31b8d-138">Trainings threads</span><span class="sxs-lookup"><span data-stu-id="31b8d-138">Training threads</span></span>

<span data-ttu-id="31b8d-139">De waarschijnlijke functie die het trainings hulpprogramma voor de classificatie is, is zeer zelden, wat inhoudt dat er meestal een groot aantal lokale optimaies is in de parameter ruimte die aanzienlijk kan variëren per kwaliteit.</span><span class="sxs-lookup"><span data-stu-id="31b8d-139">The likelihood function which is the training utility for the classifier is very seldom convex, meaning that it usually has a multitude of local optima in the parameter space that may differ significantly by quality.</span></span> <span data-ttu-id="31b8d-140">Omdat het SGD-proces kan convergeren naar slechts één specifiek optimaal, is het belang rijk om meerdere start parameter vectoren te verkennen.</span><span class="sxs-lookup"><span data-stu-id="31b8d-140">Since the SGD process can converge to only one specific optimum, it is important to explore multiple starting parameter vectors.</span></span> <span data-ttu-id="31b8d-141">Een veelvoorkomende procedure in machine learning is om dergelijke begin vectoren wille keurig te initialiseren.</span><span class="sxs-lookup"><span data-stu-id="31b8d-141">Common practice in machine learning is to initialize such starting vectors randomly.</span></span> <span data-ttu-id="31b8d-142">De Q# trainings-API accepteert een wille keurige matrix van dergelijke begin vectoren, maar deze worden sequentieel door de onderliggende code verkend.</span><span class="sxs-lookup"><span data-stu-id="31b8d-142">The Q# training API accepts an arbitrary array of such starting vectors but the underlying code explores them sequentially.</span></span> <span data-ttu-id="31b8d-143">Op een computer met meerdere kernen of in feite is het raadzaam om verschillende aanroepen naar trainings-API parallel uit te voeren Q# met verschillende parameter initialisaties in de-aanroepen.</span><span class="sxs-lookup"><span data-stu-id="31b8d-143">On a multicore computer or in fact on any parallel computing architecture it is advisable to perform several calls to Q# training API in parallel with different parameter initializations across the calls.</span></span>

#### <a name="how-to-modify-the-hyperparameters"></a><span data-ttu-id="31b8d-144">De Hyper parameters wijzigen</span><span class="sxs-lookup"><span data-stu-id="31b8d-144">How to modify the hyperparameters</span></span>

<span data-ttu-id="31b8d-145">De QML-bibliotheek is de beste manier om de Hyper parameters te wijzigen door de standaard waarden van de UDT te vervangen [`TrainingOptions`](xref:Microsoft.Quantum.MachineLearning.TrainingOptions) .</span><span class="sxs-lookup"><span data-stu-id="31b8d-145">In the QML library, the best way to modify the hyperparameters is by overriding the default values of the UDT [`TrainingOptions`](xref:Microsoft.Quantum.MachineLearning.TrainingOptions).</span></span> <span data-ttu-id="31b8d-146">Daarom noemen we het met de functie [`DefaultTrainingOptions`](xref:Microsoft.Quantum.MachineLearning.DefaultTrainingOptions) en past u de operator `w/` toe om de standaard waarden te overschrijven.</span><span class="sxs-lookup"><span data-stu-id="31b8d-146">To do this we call it with the function [`DefaultTrainingOptions`](xref:Microsoft.Quantum.MachineLearning.DefaultTrainingOptions) and apply the operator `w/` to override the default values.</span></span> <span data-ttu-id="31b8d-147">Als u bijvoorbeeld 100.000-metingen en een leer tempo van 0,01 wilt gebruiken:</span><span class="sxs-lookup"><span data-stu-id="31b8d-147">For example, to use 100,000 measurements and a learning rate of 0.01:</span></span>

```qsharp
let options = DefaultTrainingOptions()
w/ LearningRate <- 0.01
w/ NMeasurements <- 100000;
```
